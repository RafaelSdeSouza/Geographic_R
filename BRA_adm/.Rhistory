Zeros[i] ~ dpois(Zeros.mean[i])
Zeros.mean[i] <- -L[i] + C
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
y <- c(6,11,9,13,17,21,8,10,15,19,7,12)
m <- c(45,54,39,47,29,44,36,57,62,55,66,48)
x1 <- c(1,1,1,1,1,1,0,0,0,0,0,0)
x2 <- c(1,1,0,0,1,1,0,0,1,1,0,0)
x3 <- c(1,0,1,0,1,0,1,0,1,0,1,0)
noty <- m - y
bindata <-data.frame(y,m,noty,x1,x2,x3)
mybin <- glm(cbind(y,noty) ~ x1 + x2 + x3, family=binomial, data=bindata)
summary(mybin)
library(R2jags)
X <- model.matrix(~ x1 + x2 + x3, data = bindata)
K <- ncol(X)
glogit.data <- list(Y  = bindata$y/bindata$m,
N    = nrow(bindata),
X    = X,
K    = K,
m   =  m,
Zeros = rep(0, nrow(bindata))
)
sink("GLOGIT.txt")
cat("
model{
# Priors
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
# Likelihood
for (i in 1:N){
Zeros[i] ~ dpois(Zeros.mean[i])
Zeros.mean[i] <- -L[i] + C
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
sink("GLOGIT.txt")
cat("
model{
# Priors
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
# Likelihood
for (i in 1:N){
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
sink("GLOGIT.txt")
cat("
model{
# Priors
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
# Likelihood
for (i in 1:N){
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
p
noty <- m - y
m
K
sink("GLOGIT.txt")
cat("
model{
# Priors
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
# Likelihood
for (i in 1:N){
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
bindata$y/bindata$m
glogit.data <- list(Y  = y,
N    = nrow(bindata),
X    = X,
K    = K,
m   =  m
)
sink("GLOGIT.txt")
cat("
model{
# Priors
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
# Likelihood
for (i in 1:N){
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
summary(mybin)
y <- c(6,11,9,13,17,21,8,10,15,19,7,12)
m <- c(45,54,39,47,29,44,36,57,62,55,66,48)
x1 <- c(1,1,1,1,1,1,0,0,0,0,0,0)
x2 <- c(1,1,0,0,1,1,0,0,1,1,0,0)
x3 <- c(1,0,1,0,1,0,1,0,1,0,1,0)
noty <- m - y
bindata <-data.frame(y,m,noty,x1,x2,x3)
mybin <- glm(cbind(y,noty) ~ x1 + x2 + x3, family=binomial, data=bindata)
summary(mybin)
library(R2jags)
X <- model.matrix(~ x1 + x2 + x3, data = bindata)
K <- ncol(X)
glogit.data <- list(Y  = y,
N    = nrow(bindata),
X    = X,
K    = K,
m   =  m
)
sink("GLOGIT.txt")
cat("
model{
# Priors
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
# Likelihood
for (i in 1:N){
Y[i] ~ dbin(p[i],m[i])
logit(p[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
require(mcmcplots)
traplot(L0,"beta")
y <- c(6,11,9,13,17,21,8,10,15,19,7,12)
m <- c(45,54,39,47,29,44,36,57,62,55,66,48)
x1 <- c(1,1,1,1,1,1,0,0,0,0,0,0)
x2 <- c(1,1,0,0,1,1,0,0,1,1,0,0)
x3 <- c(1,0,1,0,1,0,1,0,1,0,1,0)
noty <- m - y
bindata <-data.frame(y,m,noty,x1,x2,x3)
library(R2jags)
X <- model.matrix(~ x1 + x2 + x3, data = bindata)
K <- ncol(X)
glogit.data <- list(Y  = bindata$y/bindata$m,
N    = nrow(bindata),
X    = X,
K    = K
)
sink("GLOGIT.txt")
cat("
model{
# Diffuse normal priors betas
for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}
#  Prior for theta
theta~dgamma(0.01,0.01)
for (i in 1:N){
Y[i] ~ dbeta(shape1[i],shape2[i])
shape1[i]<-theta*pi[i]
shape2[i]<-theta*(1-pi[i])
logit(pi[i]) <- eta[i]
eta[i]<-inprod(beta[],X[i,])
}
",fill = TRUE)
sink()
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta", "theta")
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 10000,
n.iter   = 20000)
print(L0, intervals=c(0.025, 0.975), digits=3)
L0 <- jags(data = glogit.data,
inits = inits,
parameters = params,
model.file = "GLOGIT.txt",
n.thin = 1,
n.chains = 3,
n.burnin = 3000,
n.iter   = 5000)
print(L0, intervals=c(0.025, 0.975), digits=3)
42000/12
200/12
30000/12
# Define Pareto distributions for Salpeter IMF
dpareto <- function(x, alpha=1.35, b=1) (alpha>0)*(b>0)^alpha / x^(alpha+1)
ppareto <- function(x, alpha=1.35, b=1) (x > b)*(1-((b>0)/x)^(alpha>0))
qpareto <- function(u, alpha=1.35, b=1) (b>0)/(1-u)^(1/(alpha>0))  # 00])
lmidpts <- log(hx$mids[counts>0])
tmp1 <- lm(ldens~lmidpts)
#   plot(lmidpts, ldens) ; abline(tmp1, col=2)
alpha.LS.hist <-  -1 - as.numeric(tmp1$coef[2])
return(alpha.LS.hist) }
alpha.LS.hist.wt <- function(x) {
hx <- hist(x, nclass=20, plot=F)
counts <- hx$counts
ldens <- log(hx$density[counts>0])
lmidpts <- log(hx$mids[counts>0])
tmp2 <- lm(ldens~lmidpts, weights=counts[counts>0])
alpha.LS.hist.wt <-  -1 - as.numeric(tmp2$coef[2])
return(alpha.LS.hist.wt)  }
six_alphas <-  function(x) {
out <- c(alpha.MLE(x), alpha.MVUE(x), alpha.mom(x),
alpha.LS.EDF(x), alpha.LS.hist(x), alpha.LS.hist.wt(x))
return(out)  }
# Comparison of alpha estimators on simulated datasets
# Construct nsim simulated datasets with npts data points and alpha=1.35
nsim=500 ;   alpha_sim = NULL
for(i in 1:nsim) {
xtmp = rpareto(npts)
alpha_sim = rbind(alpha_sim,six_alphas(xtmp))  }
colnames(alpha_sim)=c('MLE','MVUE','Moments','LS_EDF','LS_hist',
'LS_hist_wt')
# Compute mean integrated square error
bias_sim = apply(alpha_sim,2,mean) - 1.35
var_sim =  apply(alpha_sim,2,var) *(nsim-1)/nsim
mise_sim =  bias_sim^2 + var_sim
rbind(bias_sim, var_sim, mise_sim)
tmp1
dpareto <- function(x, alpha=1.35, b=1) (alpha>0)*(b>0)^alpha / x^(alpha+1)
mass <- read.table("~/Downloads/mass.txt", quote="\"", comment.char="")
View(mass)
mass
hist(mass)
mode(mass)
mass<-as.numeric(mass)
mass[1]
mass[[1]]
mass<-mass[1]
hist(mass)
mass
mass<-mass[[1]]
hist(mass)
min(mass)
max(mass)
hist(log(mass))
142000/12
50000/12
160000/12
50/12
#Program for simulating GP variates
th=1.50    #Parameter theta
da=0.25   #Parameter alpha
nx=201    #The largest computed probability. About 71 should do for most cases
ns=100    #sample size to generate
#sub-program to compute the GP probabilities. One can also include simulation here too.
f.gpd = function(th, da, nx)
{
pr=rep(0, nx)
pr[1]=exp(-th)
cc = th*exp(-da*th)
for (i in 2:nx)
{
j=i-1
c1 = (1+da/(1-da+da*j))^(j-1)
c2 = (1-da)/j + da
pr[i] = cc*c1*c2*pr[i-1]
}
if (sum(pr) < 1.0) pr[nx] = pr[nx] + (1-sum(pr))
cdf.gp = cumsum(pr)
}
cdf=f.gpd(th, da, nx)
cdf
#program part to simulate ns=100 observations from GP
xv=rep(-1, ns)
set.seed(25)
ux = runif(ns, min = 0, max = 1)  #Uniform (0, 1) is generated
for (i1 in 1:ns)
{
if (ux[i1] < cdf[1]) xv[i1]=0
for (j in 2:nx) if (cdf[j-1]<=ux[i1] && ux[i1]<cdf[j]) xv[i1]=j-1  #Inversion method to generate the variates
}
xv  #The simulated GP variates.
np=max(xv)
np
#The following will put the variates in a frequency table
variate=seq(0, np)
freq=tabulate(xv+1)
cbind(variate, freq)
sum(freq)
np
xv
hist(xv)
cdf
45000/12
library(maptools)
substitute your shapefiles here
state.map <- readShapeSpatial("BRASIL.shp")
counties.map <- readShapeSpatial("55mu2500gsd.shp")
## this is the variable we will be plotting
counties.map@data$noise <- rnorm(nrow(counties.map@data))
plot.heat <- function(counties.map,state.map,z,title=NULL,breaks=NULL,reverse=FALSE,cex.legend=1,bw=.2,col.vec=NULL,plot.legend=TRUE) {
##Break down the value variable
if (is.null(breaks)) {
breaks=
seq(
floor(min(counties.map@data[,z],na.rm=TRUE)*10)/10
,
ceiling(max(counties.map@data[,z],na.rm=TRUE)*10)/10
,.1)
}
counties.map@data$zCat <- cut(counties.map@data[,z],breaks,include.lowest=TRUE)
cutpoints <- levels(counties.map@data$zCat)
if (is.null(col.vec)) col.vec <- heat.colors(length(levels(counties.map@data$zCat)))
if (reverse) {
cutpointsColors <- rev(col.vec)
} else {
cutpointsColors <- col.vec
}
levels(counties.map@data$zCat) <- cutpointsColors
plot(counties.map,border=gray(.8), lwd=bw,axes = FALSE, las = 1,col=as.character(counties.map@data$zCat))
if (!is.null(state.map)) {
plot(state.map,add=TRUE,lwd=1)
}
##with(counties.map.c,text(x,y,name,cex=0.75))
if (plot.legend) legend("bottomleft", cutpoints, fill = cutpointsColors,bty="n",title=title,cex=cex.legend)
##title("Cartogram")
}
plot.heat(counties.map,state.map,z="noise",breaks=c(-Inf,-2,-1,0,1,2,Inf))
state.map <- readShapeSpatial("BRASIL.shp")
counties.map <- readShapeSpatial("55mu2500gsd.shp")
setwd("~/Downloads")
state.map <- readShapeSpatial("BRASIL.shp")
library(maps)
#substitute your shapefiles here
state.map <- readShapeSpatial("BRASIL.shp")
library(ggplot2)
library(maptools)
library(rgeos)
library(Cairo)
library(ggmap)
library(scales)
library(RColorBrewer)
set.seed(8000)
##set directory to the folder where the shapefile is, then input shapefile
setwd("/Users/rafael/Downloads/BRA_adm/")
states.shp <- readShapeSpatial("BRA_adm1.shp")
class(states.shp)
names(states.shp)
print(states.shp$NAME_1)
num.states<-length(states.shp$NAME_1)
mydata<-data.frame(NAME_1=states.shp$NAME_1, id=states.shp$ID_1, prevalence=rnorm(num.states, 55, 20))
head(mydata)
states.shp.f <- fortify(states.shp, region = "ID_1")
class(states.shp.f)
states.shp
states.shp.f <- fortify(states.shp, region = "ID_1")
head(mydata)
print(states.shp$ID_1)
states.shp.f <- fortify(states.shp, region = "ID_1")
install.packages("gpclib")
states.shp.f <- fortify(states.shp, region = "ID_1")
class(states.shp.f)
require(gpclib)
states.shp.f <- fortify(states.shp, region = "ID_1")
class(states.shp.f)
install.packages("gpclib", type="source")
gpclibPermit()
install.packages("gpclib", type = "source")
library(ggplot2)
library(maptools)
library(rgeos)
library(Cairo)
library(ggmap)
library(scales)
library(RColorBrewer)
require(gpclib)
gpclibPermit()
set.seed(8000)
##set directory to the folder where the shapefile is, then input shapefile
setwd("/Users/rafael/Downloads/BRA_adm/")
states.shp <- readShapeSpatial("BRA_adm1.shp")
class(states.shp)
names(states.shp)
print(states.shp$NAME_1)
num.states<-length(states.shp$NAME_1)
mydata<-data.frame(NAME_1=states.shp$NAME_1, id=states.shp$ID_1,
prevalence=rnorm(num.states, 55, 20))
head(mydata)
print(states.shp$ID_1)
states.shp.f <- fortify(states.shp, region = "ID_1")
class(states.shp.f)
merge.shp.coef<-merge(states.shp.f, mydata, by="id", all.x=TRUE)
final.plot<-merge.shp.coef[order(merge.shp.coef$order), ]
ggplot() +
geom_polygon(data = final.plot,
aes(x = long, y = lat, group = group, fill = prevalence),
color = "black", size = 0.25) +
coord_map()
